import keras
import tensorflow as tf
import os
import numpy as np
from keras.datasets import imdb
from keras.preprocessing import sequence

VOCAB_SIZE = 88584

MAXLEN = 250
BATCH_SIZE = 64

(train_data, train_labels), (test_data, test_labels) = (imdb.load_data(num_words=VOCAB_SIZE)) 

#-----Preprocessing-----#

#Padding the model 
"""We cannot pass different length data into the neural network. Each review must be of the same length. Follow the procedure below --
    - If review is grather than 250 words, trim off extra words
    - If review is less than 250 words, add the necessary ammoutn of 0's to make it equal to 250
"""

train_data = sequence.pad_sequences(train_data, MAXLEN)
test_data = sequence.pad_sequences(test_data, MAXLEN)

#-----Model creation-----#

"""We are now creating the model. We will use the word embedding layer as the first layer, then add an LSTM layer afterwards that feeds into a dense node to get our predicament statement.
   32 will be the output dimension of the vectors generated by the embedded layer. This value is changeable according to our needs or wants
"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(VOCAB_SIZE, 32),
    tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(1, activation = "sigmoid")
])

#model.summary()

#-----Model training and Compilation-----# 

model.compile(loss="binary_crossentropy", optimizer="rmsprop", metrics=['acc'])

history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)

#-----Making predictions-----#
"""The reviews are all encoded. Because of this, we need to convert any review we write into a form so that the nextwork can understand it. To do that, we will load the encodings from the 
dataset and use them to encoide our own data."""

word_index = imdb.get_word_index() #all word indexes from imdb

def encode_text(text):
    tokens = keras.preprocessing.text.text_to_word_sequence(text) #Get in some text and convert the text into tokens, which are indivdual words themselves
    tokens = [word_index[word] if word in word_index else 0 for word in tokens] #use a for loop, if the word thats in the tokens from the previous line is in the mappings list(word_index_)
    #then we will replace it's location in the list with an integer that represents an emotion. Otherwise, put 0 for neutral or unknown.
    return sequence.pad_sequences([tokens], MAXLEN)[0] #return the first index of the padded sequences.  we only want the one sequences that we have padded in thos line.

text = "that movie was just amazing, so amazing"

encoded = encode_text(text)
#print(encoded)
    
# Make a decode function as well for any future needs

reverse_word_index = {value: key for (key, value) in word_index.items()}

def decode_integers(integers):
    PAD = 0
    text = ""
    for num in integers:
        if num != PAD:
            text += reverse_word_index[num] + " "
    
    return text[:-1] #return everything except the last 'space' that was added

#print(decode_integers(encoded))

def predict(text):
    encoded_text = encode_text(text)
    pred = np.zeros((1, 250)) #blank numpy array that is in the form 1 to 250. The shape that our model expects will be in this shape, some number of entries and 250 integers representing each word
    pred[0] = encoded_text 
    result = model.predict(pred)
    print(result[0])

positive_review = "That movie was so awesome! I really loved it and would watch it again because it was amazingly great"
predict(positive_review)

negative_review = "that movie sucked. I hated it and wouldn't watch it again. Was one of the worst things i've ever watched." 
predict(negative_review)